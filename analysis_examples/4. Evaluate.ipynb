{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to know that our model is any good, we need some way of measuring its accuracy. When you're working with only a couple of features, looking at a graph can be very helpful, but once you get into higher dimensions human intuition breaks down. Thankfully, since we created this data ourselves, we have the \"ground truth\" labels available to us, and we can use that to determine how correct our learning was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'number', 'switch_time']\n",
      "['fc0ce6c8-1f2a-46e9-8870-e6bd048512e6', '974-703-1399', '0']\n"
     ]
    }
   ],
   "source": [
    "# first, read in the data\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "os.chdir('../data/')\n",
    "\n",
    "records = []\n",
    "\n",
    "with open('solutions.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        records.append(row)\n",
    "\n",
    "print(records[0]) # print the header\n",
    "records = records[1:] # remove the header\n",
    "print(records[0]) # print an example record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_labels(cluster):\n",
    "    \"\"\" given a list of phone numbers (as strings), return a list of category labels (integers)\n",
    "    that correspond to those numbers \"\"\"\n",
    "    all_people = list(set([r[0] for r in records]))\n",
    "    categories = range(len(all_people))\n",
    "    labels = []\n",
    "    for number in cluster:\n",
    "        person = [r[0] for r in records if r[1] == number]\n",
    "        if len(person) != 1:\n",
    "            raise ValueError(\"shouldn't be more or less than one person per number\")\n",
    "        person = person[0]\n",
    "        labels.append(all_people.index(person))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few different ways to measure the accuracy of your model, but for this example we'll use an Adjusted Rand Index. You can read more about this score, and other alternatives, [here](http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.6097617415991172e-05"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# retrieve our clustered data from \"3. Training\"\n",
    "%store -r all_numbers\n",
    "%store -r labels\n",
    "\n",
    "labels_true = generate_labels(all_numbers)\n",
    "\n",
    "metrics.adjusted_rand_score(labels_true, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to give you an idea, a perfect match (which we don't want! beware [overfitting](https://en.wikipedia.org/wiki/Overfitting)!) would be 1.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
